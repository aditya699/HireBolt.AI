{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m poller \u001b[38;5;241m=\u001b[39m document_analysis_client\u001b[38;5;241m.\u001b[39mbegin_analyze_document_from_url(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprebuilt-layout\u001b[39m\u001b[38;5;124m\"\u001b[39m, formUrl)\n\u001b[0;32m     28\u001b[0m result \u001b[38;5;241m=\u001b[39m poller\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m---> 29\u001b[0m \u001b[43mprint_result\u001b[49m(result)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'print_result' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code sample shows Prebuilt Layout operations with the Azure Form Recognizer client library. \n",
    "The async versions of the samples require Python 3.6 or later.\n",
    "\n",
    "To learn more, please visit the documentation - Quickstart: Document Intelligence (formerly Form Recognizer) SDKs\n",
    "https://learn.microsoft.com/azure/ai-services/document-intelligence/quickstarts/get-started-sdks-rest-api?pivots=programming-language-python\n",
    "\"\"\"\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "\"\"\"\n",
    "Remember to remove the key from your code when you're done, and never post it publicly. For production, use\n",
    "secure methods to store and access your credentials. For more information, see \n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-security?tabs=command-line%2Ccsharp#environment-variables-and-application-configuration\n",
    "\"\"\"\n",
    "endpoint = \"https://hireboltai.cognitiveservices.azure.com/\"\n",
    "key = \"da710b81451f44d5a80890e97ad2aaed\"\n",
    "\n",
    "# sample document\n",
    "formUrl = \"https://hireme.blob.core.windows.net/cvs/Aditya Bhatt.pdf\"\n",
    "\n",
    "document_analysis_client = DocumentAnalysisClient(\n",
    "    endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    ")\n",
    "    \n",
    "poller = document_analysis_client.begin_analyze_document_from_url(\"prebuilt-layout\", formUrl)\n",
    "result = poller.result()\n",
    "print_result(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aditya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyresparser import ResumeParser\n",
    "import warnings\n",
    "data = ResumeParser(\"C:/Users/aditya/Desktop/2024/HireBolt.AI/POC/Aditya Bhatt CV.pdf\").get_extracted_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R', 'Reporting', 'Excel', 'Flask', 'Key performance indicators', 'Ai', 'Data analysis', 'Datasets', 'Matplotlib', 'Github', 'Docker', 'Certification', 'Queries', 'Tensorflow', 'Healthcare', 'Logistics', 'Advertising', 'Training', 'Content', 'Presentations', 'Ggplot', 'Tableau', 'Python', 'Hypothesis', 'Coding', 'Software engineering', 'Real estate', 'Forecasting', 'Pyspark', 'Finance', 'Marketing', 'Architecture', 'Health', 'Consulting', 'Facebook', 'Database', 'Documentation', 'Pandas', 'Mysql', 'Sales', 'Engineering', 'Saas', 'Analysis', 'Pytorch', 'Sap', 'Etl', 'Budget', 'Analytics', 'Testing', 'Aws', 'Rest', 'Programming', 'Beautifulsoup', 'Interactive', 'Cash flow', 'Kpi', 'Operations', 'Statistics', 'Website', 'Sql', 'System', 'Machine learning', 'Metrics', 'Improvement', 'Presentation', 'Seaborn', 'Workflows', 'Modeling']\n"
     ]
    }
   ],
   "source": [
    "from pyresparser import ResumeParser\n",
    "\n",
    "def get_skills(filepath):\n",
    "    data = ResumeParser(filepath).get_extracted_data()\n",
    "    return data['skills']\n",
    "\n",
    "# Example usage\n",
    "skills = get_skills(\"C:/Users/aditya/Desktop/2024/HireBolt.AI/POC/Aditya Bhatt CV.pdf\")\n",
    "print(skills)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R',\n",
       " 'Reporting',\n",
       " 'Excel',\n",
       " 'Flask',\n",
       " 'Key performance indicators',\n",
       " 'Ai',\n",
       " 'Data analysis',\n",
       " 'Datasets',\n",
       " 'Matplotlib',\n",
       " 'Github',\n",
       " 'Docker',\n",
       " 'Certification',\n",
       " 'Queries',\n",
       " 'Tensorflow',\n",
       " 'Healthcare',\n",
       " 'Logistics',\n",
       " 'Advertising',\n",
       " 'Training',\n",
       " 'Content',\n",
       " 'Presentations',\n",
       " 'Ggplot',\n",
       " 'Tableau',\n",
       " 'Python',\n",
       " 'Hypothesis',\n",
       " 'Coding',\n",
       " 'Software engineering',\n",
       " 'Real estate',\n",
       " 'Forecasting',\n",
       " 'Pyspark',\n",
       " 'Finance',\n",
       " 'Marketing',\n",
       " 'Architecture',\n",
       " 'Health',\n",
       " 'Consulting',\n",
       " 'Facebook',\n",
       " 'Database',\n",
       " 'Documentation',\n",
       " 'Pandas',\n",
       " 'Mysql',\n",
       " 'Sales',\n",
       " 'Engineering',\n",
       " 'Saas',\n",
       " 'Analysis',\n",
       " 'Pytorch',\n",
       " 'Sap',\n",
       " 'Etl',\n",
       " 'Budget',\n",
       " 'Analytics',\n",
       " 'Testing',\n",
       " 'Aws',\n",
       " 'Rest',\n",
       " 'Programming',\n",
       " 'Beautifulsoup',\n",
       " 'Interactive',\n",
       " 'Cash flow',\n",
       " 'Kpi',\n",
       " 'Operations',\n",
       " 'Statistics',\n",
       " 'Website',\n",
       " 'Sql',\n",
       " 'System',\n",
       " 'Machine learning',\n",
       " 'Metrics',\n",
       " 'Improvement',\n",
       " 'Presentation',\n",
       " 'Seaborn',\n",
       " 'Workflows',\n",
       " 'Modeling']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['skills']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aditya\\anaconda3\\envs\\steam1\\lib\\site-packages\\spacy\\util.py:275: UserWarning: [W031] Model 'en_training' (0.0.0) requires spaCy v2.1 and is incompatible with the current spaCy version (2.3.9). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyresparser.resume_parser.ResumeParser object at 0x00000220AD037A30>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'ResumeParser' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskills\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m \u001b[43mget_skills\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/aditya/Desktop/2024/HireBolt.AI/POC/Aditya Bhatt CV.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mget_skills\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m ResumeParser(filepath)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(data)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskills\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'ResumeParser' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def get_skills(filepath):\n",
    "    data = ResumeParser(filepath)\n",
    "    print(data)\n",
    "    return data['skills']\n",
    "\n",
    "get_skills(\"C:/Users/aditya/Desktop/2024/HireBolt.AI/POC/Aditya Bhatt CV.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aditya\\anaconda3\\envs\\steam1\\lib\\site-packages\\spacy\\util.py:275: UserWarning: [W031] Model 'en_training' (0.0.0) requires spaCy v2.1 and is incompatible with the current spaCy version (2.3.9). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught an exception: AttributeError\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging to write to a text file\n",
    "logging.basicConfig(filename='error_log.txt', level=logging.ERROR)\n",
    "\n",
    "def get_skills(filepath):\n",
    "    try:\n",
    "        data = ResumeParser(filepath).get_extracted_data()\n",
    "        skills = data['skills'].lop\n",
    "        return skills\n",
    "    except Exception as e:\n",
    "        # Log the error to the console for debugging\n",
    "        print(f\"Caught an exception: {type(e).__name__}\")\n",
    "        \n",
    "        # Log the error to the text file\n",
    "        logging.error(f\"Error processing file '{filepath}': {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "skills = get_skills(\"C:/Users/aditya/Desktop/2024/HireBolt.AI/POC/Aditya Bhatt CV.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    print(\"open\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai \n",
    "openai.api_key='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes in string argument as parameter \n",
    "def comp(PROMPT, MaxToken=50, outputs=3): \n",
    "    # using OpenAI's Completion module that helps execute  \n",
    "    # any tasks involving text  \n",
    "    response = openai.Completion.create( \n",
    "        # model name used here is text-davinci-003 \n",
    "        # there are many other models available under the  \n",
    "        # umbrella of GPT-3 \n",
    "        model=\"text-davinci-003\", \n",
    "        # passing the user input  \n",
    "        prompt=PROMPT, \n",
    "        # generated output can have \"max_tokens\" number of tokens  \n",
    "        max_tokens=MaxToken, \n",
    "        # number of outputs generated in one call \n",
    "        n=outputs \n",
    "    ) \n",
    "    # creating a list to store all the outputs \n",
    "    output = list() \n",
    "    for k in response['choices']: \n",
    "        output.append(k['text'].strip()) \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m PROMPT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mCompute simialirity b/w \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mcomp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROMPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMaxToken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mcomp\u001b[1;34m(PROMPT, MaxToken, outputs)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcomp\u001b[39m(PROMPT, MaxToken\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m): \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# using OpenAI's Completion module that helps execute  \u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# any tasks involving text  \u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# model name used here is text-davinci-003 \u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# there are many other models available under the  \u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# umbrella of GPT-3 \u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-davinci-003\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# passing the user input  \u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPROMPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# generated output can have \"max_tokens\" number of tokens  \u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMaxToken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# number of outputs generated in one call \u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# creating a list to store all the outputs \u001b[39;00m\n\u001b[0;32m     18\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m() \n",
      "File \u001b[1;32mc:\\Users\\aditya\\anaconda3\\envs\\steam1\\lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[1;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"Compute simialirity b/w \"\"\"\n",
    "comp(PROMPT, MaxToken=3000, outputs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words(\"english\"):\n",
    "            new_text.append(\"\")\n",
    "        else:\n",
    "            new_text.append(word.strip())\n",
    "\n",
    "\n",
    "    return \" \".join(new_text).replace(\"   \",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am Aditya Bhatt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text=['I','am','Aditya','Bhatt']\n",
    "\" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" \\naditya bhatt   \\n+91 7 303041453  | adityabhatt19058568031.stats@rla.du.ac.in |linkedin |youtube |github  \\neducatio n \\n \\npg diploma in software engineering for data science    (iiit hyderabad)                                              jun 2022 - dec 2023  \\nsecured 1st position in car price prediction hackathon  (ai). portfolio 1       portfolio 2      \\nbsc(h) statistics                                                                 (delhi university )               cgpa:9.14/10      apr 2019 - apr 202 2 \\nrelevant project work : developed machine learning solution  for classification of people which would be used to allot social \\neconomic welfare to them, presented the same in ramanujan college  and won idea presentation competition. developed \\ndatabase and reporting solution for placement cell of the college.  \\n \\nexperience  \\n \\nneenopal - business analyst  july 2022 – present  \\nglobal management consulting firm with a unique and specialized focus on data science  \\n• worked with a logistics firm of sri lanka was involved in kpi gathering, data cleaning, query (sql & dax) development, \\nclient handling and creating power bi dashboards concerning to logistics, finance and volume  department of the \\ncompany. this helped them to track, close tickets and optimize their operations . \\n• worked with saas client to help them better manage their finances by developing dashboards for their income statement, \\nbalance sheet, and cash flow . the power bi dashboards were customized to their specific business needs and \\npreferences, allowing them to track their financial metrics at a granular level and  take proactive actions to maximize \\ntheir profitability . \\n• worked on mockup.ai  , created a marketplace for hr, finance  based dashboards in power bi . \\n• work ed on sales plug and play model helping companies track aov , mau, product penetration , regional analysis, top n \\nsummary, order value summary and predicting sales by integrating ai based time series forecasting (using prophet ) in \\npower bi.  \\n• creat ed business insights to optimize return on investment (roi) by interpreting key trends in web, content, and advertising \\nanalytics, and using tools such as tableau prep, tableau and power bi  for data preparation and tracking performance. \\nadditionally, identifying areas for improvement such as open rate, que, subscriptions, and cost control.  \\n• worked on developing interactive dashboards for the ga4 playbook by using bigquery and power bi to track key performance \\nindicators such as daily active users, new users, repeat orders, and revenue. additionally, conducted cohort analysis, heatmap \\nanalysis, and identified the best performing days and sources driving most of the website's traffic.  \\n• worked with a logistics company used sql,  power bi & power query  analyzing 256 variables. created a report to identify top \\nsales performers  helping them fastrack rewards to the best performers  \\n• created and deployed a power apps -based application for comment capture and manager approval, seamlessly \\nintegrated with power bi, power automate  for data analysis, enabling streamlined workflows and data -driven insights.  \\nintegrated a power virtual agents  chatbot with the same for faq section.  \\n• orchestrated a comprehensive marketing performance analysis by aggregating data from facebook, linkedin, google ads, \\ngoogle analytics, and klaviyo . utilized bigquery  to create views. the developed report, featuring visualizations and trend \\nanalyses, facilitated optimized budget allocation and contributed to enhanced marketing effectiveness.  \\n• worked on creating  stored procedures and power bi datasets  in healthcare analytics. proficient in power bi report creation, \\nwith active client management. key contributor to strategic marketing plans and backend development  for a healthcare \\ncompany.  \\n\",\n",
       " \" \\nineuron.ai  sep 2021 –dec 2021   \\nineuron .ai started as a product development company, then launched its ed -tech division.  \\n•     worked on sql queries to find insights related to sales, creating dashboards using tableau and power bi  to visualize a \\ncompany's profit and student performance, mentoring data analysis students and machine learning participants , and \\ncreating statistical presentations using python, r, and bi  tools for clients.  \\n \\nskills and proficiencies  \\n \\n• programming (python,  r, pyspark ) \\n• visualization  (power bi, tableau, matplotlib, seaborn, plotly,  ggplot, folium, excel, r, google data studio)  \\n• machine learning  (supervised and unsupervised learning, ann, cnn, time series analysis, pycaret, pytorch, tensorflow , \\nmlflow , azure ml studio ) \\n• statistics  (descriptive statistics, inferential statistics, algebra, spss, hypothesis testing, survey sampling, linear modelling)  \\n• database  (azure sql, mysql, t -sql, postgre sql,  sparksql,  mongodb, cassandra , mql ) \\n• deployment  (flask, streamlit, heroku , azure , aws sagemaker , docker ) \\n• others  (power platform , tableauprep , beautifulso up, sap crastal report , azure data factory ,azure databricks ) \\n \\n• personal projects  / poc  \\n \\n1. health mate  (heathtech) :  \\n  . developed a web app to help users get health solutions such as medical insurance prediction, diabetes and stroke \\ndetection, and happiness index.    \\n. utilized python, streamlit, heroku, cassandra (database), flask, and power bi for the development.  \\n. implemented etl, exploratory data analysis and visualizations, model training and deployed the app using streamlit and \\nheroku. the web app can help users get personalized health solutions, which would improve the user experience and \\nhelp them make informed decisions.  \\ngithub  | demo  \\n2.house price prediction  (real estate) : \\n . spearheaded a comprehensive ai project, incorporating methodologies to ensure seamless data processing and analysis. \\nutilizing pyspark , i efficiently transformed and cleansed data, while kafka facilitated data ingestion  explainable ai  \\ntechniques ensured transparent model predictions.  \\n. successfully dockerized  the project, simplifying sharing and deployment, and maintained meticulous documentation to \\nenhance project understanding. employing github for version  control.  \\n. created ai system that helps us to predict property prices with an accuracy of 85%  \\n. used pyspak,  kafka,  pandas,  pycaret,  docker, modular coding,  github  and explainable ai.  \\n           github  |technical architecture  \\n3.let india breathe  (environment science ): \\n    . leveraged pyspark on azure databricks  for data science, analyzing and modeling pm 2.5 levels across 467 files stored on \\nazure blob storage. implemented mlflow  to deploy models, creating real -time rest endpoints. this interdisciplinary approach \\naims to provide actionable insights for policymakers and communities, bridging the gap between environmental science  and ai.  \\ndemo  \\n \",\n",
       " '•  blogs:  \\n \\n         sensitivity and specificity   activation function      spatial analytics using tableau   tips & tricks for power bi  \\n \\n• certifications  / achivement s\\n \\n1.worked in analytics vidhya summer internship , worked on lead scoring using machine learning and helped marketing \\nteam increase conversions by 23 -25% and developed a python module for students.  \\n2.cleared  pl-900 microsoft certification  (power platform fu ndamental s) \\n3.cleared sql basic, intermediate, advanced  exam at hackerrank.  \\n4.python,  sql gold batch  at hackerrank.  \\n5. made five submissions in kaggle competitions ranked in top 40% in 3nd competition and ranked top 45% in 2nd \\ncompetition  \\n6. awarded employee of the quarter for q2 2023  \\n7.cleared dp -100 (microsoft certified: azure data scientist associate)   ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing Phase for beta deployment\n",
    "import logging\n",
    "import PyPDF2\n",
    "import re\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import warnings\n",
    "def get_text(filepath):\n",
    "    try:\n",
    "        reader = PdfReader(filepath)\n",
    "        content_info = []\n",
    "        no_of_pages = len(reader.pages)\n",
    "        \n",
    "        for i in range(no_of_pages):\n",
    "            page = reader.pages[i]\n",
    "            content_info.append(page.extract_text().lower())\n",
    "\n",
    "        if content_info == ['']:\n",
    "            content_info.clear()\n",
    "            content_info.append('Not able to extract')\n",
    "        \n",
    "        return content_info\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in get_text: {e}\")\n",
    "        return ['Error in extracting text']\n",
    "get_text(\"C:/Users/aditya/Desktop/2024/HireBolt.AI/POC/Aditya Bhatt CV.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_email_id(filepath: str) -> str:\n",
    "        reader = PdfReader(filepath)\n",
    "        content_info = []\n",
    "        no_of_pages = len(reader.pages)\n",
    "        \n",
    "        for i in range(no_of_pages):\n",
    "            page = reader.pages[i]\n",
    "            content_info.append(page.extract_text())\n",
    "\n",
    "            # Modified regex pattern to allow spaces in the email address\n",
    "            pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n",
    "        \n",
    "        # Using findall to get all matches in the content\n",
    "            results = re.findall(pattern, content_info[i])\n",
    "            print(results)\n",
    "        \n",
    "            # Check if there are any matches\n",
    "            if results:\n",
    "                for match in results:\n",
    "                    return match\n",
    "    \n",
    "get_email_id(\"C:/Users/aditya/Desktop/2024/HireBolt.AI/POC/Aditya Bhatt CV.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adityabhatt19058568031.stats@rla.du.ac.in'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# To get the phone number from resume\n",
    "def get_phone_number(filepath: str) -> str:\n",
    "    reader = PdfReader(filepath)\n",
    "    content_info = []\n",
    "    no_of_pages = len(reader.pages)\n",
    "    \n",
    "    for i in range(no_of_pages):\n",
    "        page = reader.pages[i]\n",
    "        content_info.append(page.extract_text())\n",
    "    \n",
    "\n",
    "        # Modified regex pattern to allow spaces in the phone number\n",
    "        pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n",
    "        \n",
    "        # Using findall to get all matches in the content\n",
    "        results = re.findall(pattern, content_info[i])\n",
    "        \n",
    "        # Check if there are any matches\n",
    "        if results:\n",
    "            for match in results:\n",
    "                return match\n",
    "        else:\n",
    "            print(\"No phone number found on page\", i + 1)\n",
    "\n",
    "# Example usage\n",
    "get_phone_number(\"C:/Users/aditya/Desktop/2024/HireBolt.AI/Data\\R1.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steam1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
